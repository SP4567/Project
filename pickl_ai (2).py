# -*- coding: utf-8 -*-
"""Pickl_Ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r4B7ys5UHpuG5rbbvgcYf74xf_ZoRAvU

# **Bank Loan Campaign**
# **Introduction**


About the Dataset-
1. ID: Customer ID
2. Pin-code: Home Area pincode
3. Age: Customer's age
4. Fam Members: Total Members in the Family
5. Education: Customer's Education
6. T-Experience: Years Of Professional Experience
7. Income: Annual Income of the Customer
8. Mortgage: Value Of the House Mortgage(if any)
9. Fixed Deposit: Does the customer have a certificate of deposit (CD) account with the bank?
10. Demat: Does the customer have a demat account with the bank?
11. Net Banking: Does the customer use internet banking facilities?
12. Loan: Did this customer accept the personal loan offered in the last campaign?

# **Use of all the libraries**

Here's a brief overview of the use of each library in your code snippet:

1. **scipy.stats**: This module is used for statistical functions. It can perform various statistical tests, probability distributions, and descriptive statistics, which are useful for analyzing data.

2. **numpy**: A fundamental package for numerical computations in Python. It provides support for arrays and matrices, along with a collection of mathematical functions to operate on these data structures.

3. **pandas**: A powerful data manipulation and analysis library. It offers data structures like DataFrames, which are ideal for handling structured data, allowing for easy data cleaning, manipulation, and analysis.

4. **matplotlib.pyplot**: A plotting library used for creating static, animated, and interactive visualizations in Python. It provides a MATLAB-like interface for plotting graphs and figures.

5. **seaborn**: Built on top of Matplotlib, Seaborn provides a high-level interface for drawing attractive statistical graphics. It simplifies the process of creating complex visualizations and enhances the aesthetics of plots.

6. **sklearn.model_selection**: This module contains tools for splitting datasets into training and testing sets, as well as for cross-validation. It helps ensure that models are trained and evaluated properly.

7. **sklearn.preprocessing**: This module includes preprocessing techniques for transforming data, such as scaling features. The `MinMaxScaler` is used to scale features to a given range, typically between 0 and 1.

8. **tensorflow**: An open-source library for numerical computation that makes machine learning faster and easier. TensorFlow provides a comprehensive ecosystem for building and deploying machine learning models.

9. **tensorflow.keras**: A high-level API for building and training deep learning models. It simplifies the process of creating neural networks.

10. **tensorflow.keras.models.Sequential**: A linear stack of layers used to build a neural network. You can easily add layers to the model in a sequential manner.

11. **tensorflow.keras.layers.Dense**: A fully connected layer in a neural network. Each neuron in this layer receives input from all neurons in the previous layer.

12. **tensorflow.keras.layers.Dropout**: A regularization technique used to prevent overfitting by randomly setting a fraction of input units to 0 during training.

13. **tensorflow.keras.layers.BatchNormalization**: A layer that normalizes the inputs to a layer for each mini-batch. This helps stabilize and accelerate the training of deep networks.

14. **tensorflow.keras.layers.Activation**: A layer that applies an activation function to the output of a previous layer, introducing non-linearity into the model.

These libraries together provide a robust framework for data analysis, preprocessing, visualization, and building machine learning models.

# **Import Libraries**
"""

from scipy import stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation

"""# **Load Data**"""

loan = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/loan_dataset_NEW.csv")

"""# **Data Exploration**"""

loan.head(5)

loan.describe()

loan.info()

"""# **Data Visualization**"""

sns.countplot(x = 'Loan', data = loan)

sns.countplot(x = 'Net Banking', data = loan, hue = 'Loan')

sns.countplot(x = 'Demat', data = loan, hue = 'Loan')

sns.countplot(x = 'Fixed Deposit', data = loan)

sns.countplot(x = 'Fixed Deposit', data = loan, hue = 'Loan')

loan.isnull().sum()

loan.isnull().value_counts()

"""# **Data Cleaning and Exploration**"""

loan = loan.drop({'Unnamed: 0', 'ID'}, axis = 1)

loan.head(5)

print("Total Mortgage:", loan['Mortgage'].sum())
print("Total Income:", loan['Income'].sum())

print("Variance of mortgage:", loan['Mortgage'].var())
print("Variance of mortgage:", loan['Income'].var())

"""# **Data Visualization**"""

plt.figure(figsize = (10, 8))
plt.subplot(2,2,1)
sns.violinplot(x = 'Loan', y = 'Mortgage', data = loan)
plt.subplot(2,2,2)
sns.violinplot(x = 'Loan', y = 'Income', data = loan)
plt.subplot(2,2,3)
sns.violinplot(x = 'Net Banking', y = 'Income', data = loan)
plt.subplot(2,2,4)
sns.violinplot(x = 'Net Banking', y = 'Mortgage', data = loan)

plt.hist(loan['age'], bins=20, color='skyblue', edgecolor='black')
plt.title('Age Distribution of Customers')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

print(stats.zscore(loan['Pin-code']))
print(stats.zscore(loan['age']))
print(stats.zscore(loan['Income']))
print(stats.zscore(loan['Mortgage']))
print(stats.zscore(loan['Fam members']))
print(stats.zscore(loan['T.Experience']))

"""# **Correlation Heatmap**"""

plt.figure(figsize=(8, 6))
sns.heatmap(loan[['age', 'Fam members', 'Income', 'Mortgage', 'T.Experience']].corr(), annot=True, cmap='icefire')
plt.show()

"""# **Performing the principal component analysis**"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
new = loan[['age', 'Fam members', 'Income', 'Mortgage', 'T.Experience']]
scaler = StandardScaler()
scaled_data = scaler.fit_transform(new)
pca = PCA(n_components=2)
pca_result = pca.fit_transform(scaled_data)
loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index = new.columns)
print(loadings)
explained_variance = pca.explained_variance_ratio_
print("Variance by each component:", explained_variance)

"""# Converting the categorical variables to the"""

loan['Loan'] = loan['Loan'].replace({'yes': 1, 'no': 0})
loan['Fixed Deposit'] = loan['Fixed Deposit'].replace({'yes': 1, 'no': 0})
loan['Demat'] = loan['Demat'].replace({'yes': 1, 'no': 0})
loan['Net Banking'] = loan['Net Banking'].replace({'yes': 1, 'no': 0})
loan['Education'] = loan['Education'].replace({'Under Graduate': 1, 'Graduate': 2, 'Post Graduate': 3})

X = loan.drop({'Loan'}, axis = 1)
y = loan['Loan']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 100)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = tf.keras.Sequential([
    Dense(64, activation='relu', input_shape = (10, )),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

plt.figure(figsize=(5, 5))
tf.keras.utils.plot_model(model, to_file = 'model.png', show_shapes = True, show_dtype = False, show_trainable = True)

"""# **Model Compile**"""

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""# **Model Training**"""

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, validation_data = (X_test, y_test))

"""# **Plotting the training history of the model**"""

hist = model.history.history
hist = pd.DataFrame(hist)
hist.plot()

"""# **Model Prediction**"""

y_predict = model.predict(X_test)
print(y_predict)

y_predict = y_predict > 0.5

print(y_predict)

"""# **Model Prediction**"""

y_train_predict = model.predict(X_train)
y_train_predict = y_train_predict > 0.5

"""# **Training data report**"""

from sklearn.metrics import confusion_matrix, classification_report
print("Training data report:\n")
cm = confusion_matrix(y_train, y_train_predict)
sns.heatmap(cm, annot = True)

"""# **Testing Data Report**"""

print("Testing data report:\n")
cm2 = confusion_matrix(y_test, y_predict)
sns.heatmap(cm2, annot = True)

"""# **Printing the classification report for the training and the testing data**"""

print("Training data report:\n", classification_report(y_train, y_train_predict))
print("Testing data report:\n", classification_report(y_test, y_predict))

model.save('loan_model.h5')

"""# **Saving the scaler**"""

import pickle
pickle.dump(scaler, open('scaler.pkl', 'wb'))

"""# **Saving the model**"""

model = keras.models.load_model('loan_model.h5')
scaler = pickle.load(open('scaler.pkl', 'rb'))

"""# **Deployment Function and the Sample JSON**"""

sample_json = {
    'Pin-code': 110001,
    'age': 20,
    'Fam members': 4,
    'Education': 4,
    'T.Experience': 5,
    'Income': 100000,
    'Mortgage': 20000,
    'Fixed Deposit': 1,
    'Demat': 1,
    'Net Banking': 1
}

def return_prediction(model, scaler, sample_json):
    pin = sample_json['Pin-code']
    age = sample_json['age']
    fm = sample_json['Fam members']
    edu = sample_json['Education']
    exp = sample_json['T.Experience']
    inc = sample_json['Income']
    mo = sample_json['Mortgage']
    fd = sample_json['Fixed Deposit']
    de = sample_json['Demat']
    nb = sample_json['Net Banking']
    dc = [[pin, age, fm, edu, exp, inc, mo, fd, de, nb]]
    dc = scaler.transform(dc)
    predictions = model.predict(dc)
    classes = np.argmax(predictions, axis = 1)
    return classes
return_prediction(model, scaler, sample_json)