# -*- coding: utf-8 -*-
"""Pickl_Ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r4B7ys5UHpuG5rbbvgcYf74xf_ZoRAvU
"""

from scipy import stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation

loan = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/loan_dataset_NEW.csv")

loan.head(5)

loan.describe()

loan.info()

sns.countplot(x = 'Loan', data = loan)

sns.countplot(x = 'Net Banking', data = loan, hue = 'Loan')

sns.countplot(x = 'Demat', data = loan, hue = 'Loan')

sns.countplot(x = 'Fixed Deposit', data = loan)

sns.countplot(x = 'Fixed Deposit', data = loan, hue = 'Loan')

loan.isnull().sum()

loan.isnull().value_counts()

loan = loan.drop({'Unnamed: 0', 'ID'}, axis = 1)

loan.head(5)

print("Total Mortgage:", loan['Mortgage'].sum())
print("Total Income:", loan['Income'].sum())

print("Variance of mortgage:", loan['Mortgage'].var())
print("Variance of mortgage:", loan['Income'].var())

plt.figure(figsize = (10, 8))
plt.subplot(2,2,1)
sns.violinplot(x = 'Loan', y = 'Mortgage', data = loan)
plt.subplot(2,2,2)
sns.violinplot(x = 'Loan', y = 'Income', data = loan)
plt.subplot(2,2,3)
sns.violinplot(x = 'Net Banking', y = 'Income', data = loan)
plt.subplot(2,2,4)
sns.violinplot(x = 'Net Banking', y = 'Mortgage', data = loan)

plt.hist(loan['age'], bins=20, color='skyblue', edgecolor='black')
plt.title('Age Distribution of Customers')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

print(stats.zscore(loan['Pin-code']))
print(stats.zscore(loan['age']))
print(stats.zscore(loan['Income']))
print(stats.zscore(loan['Mortgage']))
print(stats.zscore(loan['Fam members']))
print(stats.zscore(loan['T.Experience']))

plt.figure(figsize=(8, 6))
sns.heatmap(loan[['age', 'Fam members', 'Income', 'Mortgage', 'T.Experience']].corr(), annot=True, cmap='icefire')
plt.show()

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
new = loan[['age', 'Fam members', 'Income', 'Mortgage', 'T.Experience']]
scaler = StandardScaler()
scaled_data = scaler.fit_transform(new)
pca = PCA(n_components=2)
pca_result = pca.fit_transform(scaled_data)
loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index = new.columns)
print(loadings)
explained_variance = pca.explained_variance_ratio_
print("Variance by each component:", explained_variance)

loan['Loan'] = loan['Loan'].replace({'yes': 1, 'no': 0})
loan['Fixed Deposit'] = loan['Fixed Deposit'].replace({'yes': 1, 'no': 0})
loan['Demat'] = loan['Demat'].replace({'yes': 1, 'no': 0})
loan['Net Banking'] = loan['Net Banking'].replace({'yes': 1, 'no': 0})
loan['Education'] = loan['Education'].replace({'Under Graduate': 1, 'Graduate': 2, 'Post Graduate': 3})

X = loan.drop({'Loan'}, axis = 1)
y = loan['Loan']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 100)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = tf.keras.Sequential([
    Dense(64, activation='relu', input_shape = (10, )),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

plt.figure(figsize=(5, 5))
tf.keras.utils.plot_model(model, to_file = 'model.png', show_shapes = True, show_dtype = False, show_trainable = True)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, validation_data = (X_test, y_test))

hist = model.history.history
hist = pd.DataFrame(hist)
hist.plot()

y_predict = model.predict(X_test)
print(y_predict)

y_predict = y_predict > 0.5

print(y_predict)

y_train_predict = model.predict(X_train)
y_train_predict = y_train_predict > 0.5

from sklearn.metrics import confusion_matrix, classification_report
print("Training data report:\n")
cm = confusion_matrix(y_train, y_train_predict)
sns.heatmap(cm, annot = True)

print("Testing data report:\n")
cm2 = confusion_matrix(y_test, y_predict)
sns.heatmap(cm2, annot = True)

print("Training data report:\n", classification_report(y_train, y_train_predict))
print("Testing data report:\n", classification_report(y_test, y_predict))

model.save('loan_model.h5')

import pickle
pickle.dump(scaler, open('scaler.pkl', 'wb'))

model = keras.models.load_model('loan_model.h5')
scaler = pickle.load(open('scaler.pkl', 'rb'))

sample_json = {
    'Pin-code': 110001,
    'age': 20,
    'Fam members': 4,
    'Education': 4,
    'T.Experience': 5,
    'Income': 100000,
    'Mortgage': 20000,
    'Fixed Deposit': 1,
    'Demat': 1,
    'Net Banking': 1
}

def return_prediction(model, scaler, sample_json):
    pin = sample_json['Pin-code']
    age = sample_json['age']
    fm = sample_json['Fam members']
    edu = sample_json['Education']
    exp = sample_json['T.Experience']
    inc = sample_json['Income']
    mo = sample_json['Mortgage']
    fd = sample_json['Fixed Deposit']
    de = sample_json['Demat']
    nb = sample_json['Net Banking']
    dc = [[pin, age, fm, edu, exp, inc, mo, fd, de, nb]]
    dc = scaler.transform(dc)
    predictions = model.predict(dc)
    classes = np.argmax(predictions, axis = 1)
    return classes
return_prediction(model, scaler, sample_json)